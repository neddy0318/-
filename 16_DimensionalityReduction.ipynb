{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "## 1. 개요\n",
    "\n",
    "## 2. 종류\n",
    "\n",
    "### 1)주성분 분석(PCA, Principal Component Analysis)\n",
    "\n",
    "&nbsp;&nbsp; (1)개요\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - PCA는 대표적인 차원 축소 방식으로, 피처들 간의 상관관계를 이용해 주성분을 추출해내는 방법입니다. 기존 모든 변수들의 선형 결합으로 주성분을 만들어 내며, 이를 간단하게 도식화하면 다음과 같습니다.\n",
    "\n",
    "$$\n",
    "c = a*x_{1} + b*x_{2}\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;  - PCA는 기존 데이터 차원(feature) 값을 결합해 새로운 피처를 추출하기에 피처 추출에 속합니다. 이를 통해 더 적은 차원으로 데이터를 효과적으로 설명해내는 것이 기본적인 목적입니다.\n",
    "\n",
    "\n",
    "### 3. PCA 절차\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; 1)데이터의 분산을 최대한 보존하면서 직교하는 새로운 축(기저)을 찾아냅니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -분산이 커야하는 이유 : 원본의 특징을 최대한 반영하게 될 수 있을 분만 아니라 데이터들 사이의 차이가 명확해져 좋은 모델을 만들 수 있습니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -공분산 행렬의 고유값 분해를 통해 새로운 축을 알아낼 수 있습니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; 2)첫번째 축과 직교(독립)하면서 분산이 최대인 두번째 축을 찾습니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; 3)위의 과정을 기존 차원의 수만큼 반복합니다. 이를 통해 기존 차원 수만큼 주성분을 찾아냅니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -주의 : 첫번째 축과 세번째 축은 서로 차원이 다름!\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; 4)원 데이터의 분산을 얼마나 반영하는지 확인하면서 적정 수의 주성분을 선택합니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; 5)새로운 차원에 대해 데이터를 사영(좌표반환) 해줍니다. 앞의 도식을 생각하면 됩니다.\n",
    "\n",
    "$$\n",
    "c = a*x_{1} + b*x_{2}\n",
    "$$\n",
    "\n",
    "### 2)독립 성분 분석(ICA, Independent Component Analysis)\n",
    "\n",
    "&nbsp;&nbsp; (1)개요\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - ICA는 주로 데이터의 숨겨진 독립적인 성분을 추출하기 위한 방법입니다. ICA는 데이터의 통계적 독립성을 기반으로 하여, 관측된 데이터가 여러 개의 독립적인 원천에서 생성되었다고 가정하고, 이 독립적인 원천 성분을 복원하려고 합니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - ICA의 기본 개념 : 혼합 모델(Mixing Model):\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - ICA는 데이터가 독립적인 원천 성분(예: 신호, 특징 등)의 혼합으로 구성되어 있다고 가정합니다. 이 혼합된 데이터는 다음과 같은 형태로 표현될 수 있습니다;\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 𝑋 = 𝐴𝑆\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 𝑋는 관측된 데이터, \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 𝐴는 혼합 행렬,\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 𝑆는 독립적인 원천 성분입니다.\n",
    "\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - ICA의 목표는 혼합된 데이터 𝑋에서 원천 성분 𝑆을 추출하는 것입니다. 이를 위해 혼합 행렬 𝐴를 추정하고, 그 역행렬을 이용하여 독립적인 성분을 복원합니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - 복원된 성분들은 서로 통계적으로 독립적이며, 비가우시안(비정규분포)인 경우가 많습니다.\n",
    "\n",
    "### 3)선형 판별 분석(LDA, Linear Discriminant Analysis)\n",
    "\n",
    "![](https://velog.velcdn.com/images%2Fswan9405%2Fpost%2Fa916d72e-8235-4103-9234-bf0cec327baa%2Fimage.png)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; 출처 : https://velog.velcdn.com/images%2Fswan9405%2Fpost%2Fa916d72e-8235-4103-9234-bf0cec327baa%2Fimage.png\n",
    "\n",
    "&nbsp;&nbsp; (1)개요\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - 분류 모델에 사용하는 차원 축소 기법으로, 데이터 세트의 클래스 내 분산을 최소로 하고, 클래스 간 분산을 최대로 하는 벡터를 찾아 차원 축소를 하는 방식입니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - 클래스 간 분산과 클래스 내 분산을 최대한 분리하기 위해 Fisher's Criterion(분리도 기준)을 사용하여 변환 행렬을 찾습니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - 얼굴 인식, 이미지 인식, 문서 분류 등 다양한 분야에 사용됩니다.\n",
    "\n",
    "&nbsp;&nbsp; (2)절차\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; (a)데이터의 전체 평균과 각 클래스의 평균을 계산합니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; (b)클래스 간 분산 행렬(S_B)와 클래스 내 분산 행렬(S_W)을 계산합니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; (c)S_W^(-1) * S_B의 고유값과 고유벡터를 계산하여, 가장 큰 고유값에 해당하는 고유벡터를 선택합니다. 이 고유벡터들이 새로운 저차원 공간의 축이 됩니다.\n",
    "\n",
    "### 4)주파수 도메인 분석(FDA, Frequency Domain Analysis)\n",
    "\n",
    "&nbsp;&nbsp; (1)개요\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - 주파수 도메인 분석은 신호 처리 분야에서 시간 도메인에서의 신호를 주파수 도메인으로 변환하여 분석하는 방법입니다. 주로 푸리에 변환을 사용하며, 시계열 데이터 분석 및 이미지 처리, 오디오 처리 등에 활용할 수 있습니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - 주파수 도메인 분석 자체가 차원 축소 기법은 아니지만, 주파수 성분 중 에너지가 높은 주요 성분만 선택하거나, 특정 주파수 대역을 필터링하여 사용하는 식으로 데이터의 차원을 줄일 수 있습니다.\n",
    "\n",
    "### 5)잠재 의미 분석(LSA, Latent Semantic Analysis)\n",
    "\n",
    "&nbsp;&nbsp; (1)개요\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - 특이값 분해(SVD; Singular Value Decomposition)을 활용하여 문서에 함축된 주제를 찾아내는 찾아 내는 차원 축소 기법으로, 데이터가 클 수록 차원 축소의 효과가 크다는 장점이 있습니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - 문서에 포함된 단어가 정규 분포를 따라야 한다는 단점이 있습니다.\n",
    "\n",
    "\n",
    "### 6)특이값 분해(SVD, Singular Value Decomposition)\n",
    "\n",
    "&nbsp;&nbsp; (1)개요\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - 선형 대수에서 중요한 기법으로, 주어진 행렬을 세 개의 특수한 행렬로 분해하는 방법입니다. 이 기법은 데이터 분석, 차원 축소, 추천 시스템 등 여러 분야에서 널리 사용됩니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - 특이값 분해 자체는 차원축소는 아니지만, 다음의 방식으로 차원 축소에 사용할 수 있습니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; (a)상위 특이값 선택\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - 특이값 행렬 Σ에서 상위 k개의 특이값과 해당하는 특이벡터만을 선택합니다. 여기서 k는 축소할 차원의 수입니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; (b)저차원 근사 생성\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - 선택된 특이값과 특이벡터를 사용하여 원래의 데이터 행렬을 저차원으로 근사합니다. 즉,\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A_k = U_k Σ_k V_k^T\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 를 계산하여 데이터의 중요한 구조를 유지하면서 차원을 줄입니다.\n",
    "\n",
    "\n",
    "\n",
    "&nbsp;&nbsp; (2)식 및 해설\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - 특이값 분해는 임의의 $m \\times n$ 행렬 $A$를 다음과 같은 형태로 분해합니다.\n",
    "\n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - **$U$**: $m \\times m$ 직교 행렬\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - **$\\Sigma$**: $m \\times n$ 대각 행렬 (대각 원소는 특이값)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - **$V^T$**: $n \\times n$ 직교 행렬의 전치 행렬\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; * 구성 요소\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - **$U$ (좌측 특이 행렬)**:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - $A$의 열 공간을 나타내는 직교 벡터로 구성됩니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - $U$의 각 열은 $A$의 열 공간에서의 주축을 나타냅니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; **$\\Sigma$ (특이값 행렬)**:\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - $A$의 주요 성분을 나타내는 대각 행렬입니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - 대각선 상의 값들은 **특이값**이라고 불리며, 행렬 $A$의 중요도를 나타냅니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - 일반적으로 특이값은 내림차순으로 정렬되어 있습니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; **$V^T$ (우측 특이 행렬의 전치 행렬)**:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - $A$의 행 공간을 나타내는 직교 벡터로 구성됩니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - $V$의 각 열은 $A$의 행 공간에서의 주축을 나타냅니다.\n",
    "\n",
    "### 7)t-SNE(t-Disturbuted Stochastic Neighbor Embedding)\n",
    "\n",
    "&nbsp;&nbsp; (1)개요\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - 고차원의 데이터를 저차원 공간에 차원 축소하는 방법으로, 고차원 데이터를 시각화하는데 주로 사용합니다. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - t-SNE 과정이 끝나면 초기의 피처를 확인하기 어렵고 그렇기에 무언가를 추론하기 어렵다는 단점이 있습니다. 이런 이유로 시각화와 군집분석에 주로 사용합니다.\n",
    "\n",
    "&nbsp;&nbsp; (2)방법\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;  **고차원 공간의 유사성**:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;  - 데이터 포인트 간의 유사성을 확률적 거리 측정 방법을 사용하여 계산합니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;  - **조건부 확률**: 각 데이터 포인트 \\( i \\)에서 다른 데이터 포인트 \\( j \\)까지의 조건부 확률 \\( p_{ij} \\)를 계산합니다. 이는 데이터 포인트 \\( i \\) 주위의 지역적 밀도를 나타냅니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;  - **가우시안 분포**: 데이터 포인트 \\( i \\)의 주변에 있는 데이터 포인트 \\( j \\)는 가우시안 분포를 따라 확률적으로 측정됩니다.\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; **저차원 초기화**:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;  - 저차원 공간 (예: 2D 또는 3D)에서 각 데이터 포인트를 임의로 초기화합니다. 초기화는 일반적으로 무작위로 수행됩니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - **저차원 공간의 유사성 계산**:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - 저차원 공간에서 데이터 포인트 간의 유사성을 계산합니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - **Student's t-분포**: 저차원에서의 유사성은 Student's t-분포를 사용하여 계산합니다. 이는 고차원에서의 유사성을 저차원에서 잘 보존하기 위한 방법입니다.\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **비용 함수 정의**:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - 고차원 유사성과 저차원 유사성 간의 차이를 측정하는 비용 함수를 정의합니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - **Kullback-Leibler Divergence (KL Divergence)**: 일반적으로 KL 발산을 사용하여 고차원 유사성과 저차원 유사성 간의 차이를 측정합니다.\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - **최적화**:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - 비용 함수를 최소화하기 위해 최적화 알고리즘을 사용하여 저차원 좌표를 조정합니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - **경사 하강법**: 비용 함수를 최적화하기 위해 경사 하강법 (Gradient Descent) 또는 다른 최적화 기법을 사용합니다.\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **시각화**:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - 저차원으로 축소된 데이터 포인트를 시각화합니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - 2D 또는 3D 플롯을 생성하여 데이터의 군집 구조나 패턴을 분석합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 8)UMAP(Uniform Manifold Approximation and Projection)\n",
    "\n",
    "&nbsp;&nbsp; (1)개요\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - 데이터의 고차원 매니폴드(다양한 차원의 공간) 구조를 저차원 공간에서 보존하는 방식으로, 데이터 시각호, 전처리, 군집 분석 등에 다양하게 사용합니다. 매니폴드는 고차원 공간에서 낮은 차원의 구조를 형성하는 데이터의 내재적 구조를 의미합니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - 차원 축소시 데이터의 이웃 관계를 보존하는 하는 것이 중요합니다. 고차원에서 이웃을 찾을 때 고차원 구조를 잘 반영하는 확률적 그래프를 구성하고, 이를 토대로 저차원 공간에서의 이웃 관계와 기존 이웃관계 간 오차에 대한 비용 함수가 최소화되도록 데이터 구조를 최적화합니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - 고차원 데이터에서 '퍼플렉시티(perplexity, 데이터의 지역적 밀도)'를 고려해 이웃을 찾고, 유클리드 거리 등으로 이웃을 정의합니다.\n",
    "\n",
    "&nbsp;&nbsp; (2)장단점\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - 빠르게 수행할 수 있으며, 군집이 잘 보존됩니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - 하이퍼파라미터의 영향을 많이 받습니다.\n",
    "\n",
    "### 9)Autoencoders\n",
    "\n",
    "&nbsp;&nbsp; (1)개요\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; - 신경망 기반의 차원 축소 기법으로, 데이터의 중요한 특징을 학습하여 원본 데이터를 효율적으로 압축하고 복원합니다.\n",
    "\n",
    "&nbsp;&nbsp; (2)구조 : 오토인코더는 두 개의 주요 네트워크로 구성됩니다:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; (a)인코더(Encoder):\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - 입력층(Input Layer): 원본 데이터를 입력받습니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - 인코딩층(Encoding Layer): 입력 데이터를 더 낮은 차원의 잠재 공간(latent space)으로 변환합니다. 이 과정에서 데이터의 중요한 정보만을 추출하려고 합니다. 예를 들어, 100차원의 데이터를 20차원으로 압축할 수 있습니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; -잠재 벡터(Latent Vector): 인코더의 출력으로, 데이터의 축소된 표현입니다. 이 벡터는 데이터의 본질적인 특징을 담고 있습니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; (b)디코더(Decoder):\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - 디코딩층(Decoding Layer): 잠재 벡터를 원본 데이터와 같은 차원으로 복원합니다. 이 단계에서는 인코더에서 추출된 정보만을 바탕으로 원본 데이터와 유사한 출력을 생성합니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - 출력층(Output Layer): 최종적으로 원본 데이터와 유사한 형태의 복원된 데이터를 출력합니다.\n",
    "\n",
    "&nbsp;&nbsp; (3)학습 과정 : 오토인코더는 주로 다음과 같은 방식으로 학습됩니다:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - 손실 함수(Loss Function): 오토인코더는 원본 입력과 디코더의 출력 간의 차이를 최소화하도록 학습합니다. 일반적으로 평균 제곱 오차(Mean Squared Error, MSE)나 교차 엔트로피(Cross-Entropy) 같은 손실 함수를 사용하여 두 데이터 간의 유사성을 평가합니다.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; - 역전파(Backpropagation): 손실 함수의 값에 따라 인코더와 디코더의 가중치를 조정하여 모델의 성능을 개선합니다. 이 과정에서 경량화된 잠재 벡터가 원본 데이터를 잘 재구성할 수 있도록 학습합니다.\n",
    "\n",
    "\n",
    "## 3. 참고자료\n",
    "\n",
    "&nbsp;&nbsp; LDA : https://velog.io/@swan9405/LDA-Linear-Discriminant-Analysis\n",
    "\n",
    "&nbsp;&nbsp; LSA : https://5bluewhale.tistory.com/14\n",
    "\n",
    "&nbsp;&nbsp; SVD : https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-20-%ED%8A%B9%EC%9D%B4%EA%B0%92-%EB%B6%84%ED%95%B4Singular-Value-Decomposition\n",
    "\n",
    "&nbsp;&nbsp; t-SNE : https://gaussian37.github.io/ml-concept-t_sne/\n",
    "\n",
    "&nbsp;&nbsp; UMAP : https://kwonkai.tistory.com/65\n",
    "\n",
    "&nbsp;&nbsp; Autoencoder : https://hyunsooworld.tistory.com/entry/%EC%98%A4%ED%86%A0%EC%9D%B8%EC%BD%94%EB%8D%94Autoencoder%EA%B0%80-%EB%AD%90%EC%97%90%EC%9A%94-1Dimension-reduction-and-Maninfold-Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
